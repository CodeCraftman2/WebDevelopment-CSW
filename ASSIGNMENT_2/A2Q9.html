<!DOCTYPE html>
<html>
<head>
    <body>
        <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    </body>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background: whitesmoke;
    }

    .header {
      background: gray;
      color: white;
      text-align: center;
      padding: 10px 0;
      position: sticky;
      top: 0;
      z-index: 2;
    }

    .nav {
      display: flex;
      background: #333;
      padding: 10px 0;
      position: sticky;
      top: 0;
      z-index: 1;
    }

    .nav a {
      color: white;
      text-decoration: none;
      padding: 10px 20px;
      transition: background-color 0.8s;
    }

    .nav a:hover {
      background-color: gray;
    }

    .image {
      background-image: url('Taj Mahal.jpg');
      background-size: cover;
      background-position: center;
      height: 800px;
      width: auto;
    }

    .content {
      color: darkslategrey;
      padding: 20px;
    }

    .text {
      text-align: center;
      font-size: 24px;
      padding: 20px;
    }
  </style>
</head>
<body>
  <div class="header">
    <h1>Your Website</h1>
  </div>

  <div class="image"></div>

  <div class="nav">
    <a href="#">Home</a>
    <a href="#">About Us</a>
    <a href="#">Services</a>
    <a href="#">Contact</a>
  </div>

  <div class="content">
    <div class="text">
      <h2>Welcome to Our Website</h2>
      <p>In the realm of technology, computers stand as one of the most significant inventions in human history. These versatile machines, born from a legacy of inventors and innovators, have shaped the way we live, work, and connect with the world. The journey of computers from their rudimentary beginnings to the complex devices we use today is nothing short of extraordinary.
        
        Computer history can be traced back to ancient times when the concept of mechanical calculation was first conceived. Early devices like the abacus and the Antikythera mechanism displayed an early human interest in automating processes and solving complex problems. However, it wasn't until the 19th and 20th centuries that the foundations of modern computing were laid.
        
        The pivotal moment arrived with the work of Charles Babbage, a British mathematician and inventor. Babbage's designs for the "Analytical Engine" laid the groundwork for the concept of a programmable machine. Although his invention was never fully realized in his time, it served as the blueprint for future generations of computers.
        
        The 20th century witnessed a rapid evolution in computing technology. In the 1930s and 1940s, pioneers like Alan Turing and Konrad Zuse developed early computers and laid the groundwork for the development of electronic digital computers. The first electronic digital computer, the Electronic Numerical Integrator and Computer (ENIAC), was unveiled in the 1940s. ENIAC was a colossal machine, taking up an entire room and consuming an immense amount of power. Still, it marked a significant leap forward in computing capabilities.
        
        As technology advanced, computers became smaller, more powerful, and more accessible. The invention of the transistor in the late 1940s paved the way for a new era of computing. Transistors replaced vacuum tubes and made computers more reliable and energy-efficient. This breakthrough led to the development of the first commercially successful computer, the UNIVAC I, which was used for tasks like weather prediction and data analysis.
        
        The 1970s and 1980s saw the advent of the personal computer revolution. Pioneering companies like Apple and IBM introduced home computers that brought computing power to the masses. The graphical user interface (GUI) and the mouse, developed at Xerox's Palo Alto Research Center, made computers more user-friendly. Microsoft's Windows operating system and Apple's Macintosh further popularized personal computing.
        
        With the rise of the internet in the 1990s, computers became indispensable tools for communication and information access. The World Wide Web, created by Tim Berners-Lee, opened up a new frontier for global connectivity. E-commerce, social networking, and online information exchange became integral parts of our daily lives, all powered by computers.
        
        In the 21st century, the ubiquity of computers continues to expand. Mobile devices, including smartphones and tablets, have become indispensable companions, granting us access to information, entertainment, and services on the go. Cloud computing, which allows us to store and access data remotely, has revolutionized the way we work and collaborate.
        
        The computational power of computers has also grown exponentially. Modern computers are equipped with multi-core processors, high-performance graphics cards, and vast amounts of memory. This increased computing power has enabled breakthroughs in fields like artificial intelligence, scientific research, and entertainment.
        
        Artificial intelligence, or AI, represents one of the most exciting frontiers in computer science. Machine learning algorithms, powered by extensive data and powerful hardware, can perform tasks once thought to be the exclusive domain of humans. AI has applications in speech recognition, image analysis, autonomous vehicles, and more, and it continues to evolve.
        
        In addition to the ongoing evolution of computing technology, ethical and societal considerations come to the forefront. Issues related to privacy, cybersecurity, and the responsible use of AI demand our attention and careful deliberation.
        
        In conclusion, computers have come a long way from their humble beginnings as mechanical calculators. They have evolved into sophisticated and ubiquitous tools that have transformed our world. As we look to the future, it's clear that computers will remain at the heart of technological progress, continuing to shape our lives and the world we inhabit.</p>
    </div>
  </div>
</body>
</html>
